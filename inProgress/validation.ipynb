{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607b7b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning\n",
      "  Using cached pytorch_lightning-1.7.0-py3-none-any.whl (700 kB)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (2022.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (4.1.0)\n",
      "Collecting pyDeprecate>=0.3.1\n",
      "  Using cached pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (21.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (6.0)\n",
      "Collecting tensorboard>=2.9.1\n",
      "  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Using cached torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.9.* in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (1.10.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=17.0->pytorch_lightning) (3.0.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.3.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (2.0.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.19.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.35.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (59.8.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.43.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from absl-py>=0.4->tensorboard>=2.9.1->pytorch_lightning) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (4.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.11)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (3.2.0)\n",
      "Installing collected packages: pyDeprecate, torchmetrics, tensorboard, pytorch_lightning\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.6.0\n",
      "    Uninstalling tensorboard-2.6.0:\n",
      "      Successfully uninstalled tensorboard-2.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.6.2 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.9.1 which is incompatible.\n",
      "tensorflow 2.6.2 requires typing-extensions~=3.7.4, but you have typing-extensions 4.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyDeprecate-0.3.2 pytorch_lightning-1.7.0 tensorboard-2.9.1 torchmetrics-0.9.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "febab8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting disent==0.4.0\n",
      "  Using cached disent-0.4.0-py3-none-any.whl (340 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in /opt/conda/lib/python3.9/site-packages (from disent==0.4.0) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.9/site-packages (from disent==0.4.0) (1.19.5)\n",
      "Collecting torch-optimizer>=0.1.0\n",
      "  Using cached torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: tqdm>=4.60.0 in /opt/conda/lib/python3.9/site-packages (from disent==0.4.0) (4.62.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from disent==0.4.0) (1.8.0)\n",
      "Requirement already satisfied: pytorch-lightning>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from disent==0.4.0) (1.7.0)\n",
      "Requirement already satisfied: h5py>=2.10.0 in /opt/conda/lib/python3.9/site-packages (from disent==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.9/site-packages (from disent==0.4.0) (1.10.2)\n",
      "Requirement already satisfied: pip>=21.0 in /opt/conda/lib/python3.9/site-packages (from disent==0.4.0) (22.0.3)\n",
      "Requirement already satisfied: torchvision>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from disent==0.4.0) (0.11.3)\n",
      "Requirement already satisfied: Pillow>=8.2.0 in /opt/conda/lib/python3.9/site-packages (from disent==0.4.0) (9.0.1)\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning>=1.4.0->disent==0.4.0) (0.3.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning>=1.4.0->disent==0.4.0) (6.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning>=1.4.0->disent==0.4.0) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning>=1.4.0->disent==0.4.0) (4.1.0)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning>=1.4.0->disent==0.4.0) (2.9.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning>=1.4.0->disent==0.4.0) (0.9.3)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning>=1.4.0->disent==0.4.0) (2022.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.24.2->disent==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.24.2->disent==0.4.0) (1.1.0)\n",
      "Collecting pytorch-ranger>=0.1.1\n",
      "  Using cached pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=17.0->pytorch-lightning>=1.4.0->disent==0.4.0) (3.0.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (3.3.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (0.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (59.8.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (1.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (1.35.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (0.4.6)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (3.19.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from absl-py>=0.4->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (4.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->disent==0.4.0) (21.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->disent==0.4.0) (3.2.0)\n",
      "Installing collected packages: pytorch-ranger, torch-optimizer, disent\n",
      "Successfully installed disent-0.4.0 pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install disent==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983f96b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (4.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a32392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0061119-9f08-4cc5-8027-19ff4631ae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (0.13.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.26)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.9.2)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb) (1.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.8.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a91068-8970-48fc-8d0c-3b9ec10e5d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220807_014531-116fccty</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/116fccty\" target=\"_blank\">sage-wildflower-5</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09380ac49634420fa59170a52a4a3e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.95875, 'dci.informativeness_test': 0.1855, 'dci.disentanglement': 0.009197158716181066, 'dci.completeness': 0.00750449279484891, 'mig.discrete_score': 0.008437209629986486, 'sap.score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=2, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c545c7fc-1042-4dcc-b8ba-97e83a2c1653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:116fccty) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>kl_loss</td><td>▅▁▁▁█▂▂▂▂▄▂▁▁▁▂▁▁▁▁▁▂▁▁▂▁▁█▃▂▅▁▃▂▁▁▂▁▁▁▁</td></tr><tr><td>kl_reg_loss</td><td>▅▁▁▁█▂▂▂▂▄▂▁▁▁▂▁▁▁▁▁▂▁▁▂▁▁█▃▂▅▁▃▂▁▁▂▁▁▁▁</td></tr><tr><td>loss</td><td>▄▅▂▄▅▃▄▅▅▁▄▄▂▃▇▅▄▅▅▃▆▆▃▃▃▃▅▃▃▅▃█▅▅▄▅▅▃▄▄</td></tr><tr><td>pixel_loss</td><td>▄▅▂▄▅▃▄▅▅▁▄▄▂▃▇▅▄▅▅▃▆▆▃▃▃▃▅▃▃▅▃█▅▅▄▅▅▃▄▄</td></tr><tr><td>recon_loss</td><td>▄▅▂▄▅▃▄▅▅▁▄▄▂▃▇▅▄▅▅▃▆▆▃▃▃▃▅▃▃▅▃█▅▅▄▅▅▃▄▄</td></tr><tr><td>reg_loss</td><td>▅▁▁▁█▂▂▂▂▄▂▁▁▁▂▁▁▁▁▁▂▁▁▂▁▁█▃▂▅▁▃▂▁▁▂▁▁▁▁</td></tr><tr><td>shared</td><td>▄▅▅▅▄▄▅▄▅▅▅▄▄▆▂▇▅▆▄▃▄▅▁▂▄▂▇▃▇██▁▅▅▅▄▅▅▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>0.0</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>kl_loss</td><td>0.0</td></tr><tr><td>kl_reg_loss</td><td>0.0</td></tr><tr><td>loss</td><td>88.36744</td></tr><tr><td>pixel_loss</td><td>88.36743</td></tr><tr><td>recon_loss</td><td>88.36743</td></tr><tr><td>reg_loss</td><td>0.0</td></tr><tr><td>shared</td><td>3.89844</td></tr><tr><td>trainer/global_step</td><td>5849</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sage-wildflower-5</strong>: <a href=\"https://wandb.ai/iceburg/uncategorized/runs/116fccty\" target=\"_blank\">https://wandb.ai/iceburg/uncategorized/runs/116fccty</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220807_014531-116fccty/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:116fccty). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220807_034451-2j6ounle</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2j6ounle\" target=\"_blank\">fluent-morning-6</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cbe48cd17647efa1d01616565119c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.9715, 'dci.informativeness_test': 0.729, 'dci.disentanglement': 0.4678957277868262, 'dci.completeness': 0.476518398132732, 'mig.discrete_score': 0.3479721196722052, 'sap.score': 0.010299999999999997}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=1, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4459a8-0c0e-4940-b4b7-6d4792f49104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220807_160255-1jbmr3p4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1jbmr3p4\" target=\"_blank\">polished-feather-8</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e721a7db7445c582cd40d7ee937035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.962, 'dci.informativeness_test': 0.217, 'dci.disentanglement': 0.0038403137500412575, 'dci.completeness': 0.004091004740570697, 'mig.discrete_score': 0.0033430075155543855, 'sap.score': 9.99999999999994e-05}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=4, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d3b9d3-3197-4047-b2f1-0f7904cff5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3qdhbbfg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆█</td></tr><tr><td>kl_loss</td><td>▁▁▁▁▁▁▁▁▅▅▅▅▆▅▅▅▆▅▆▆▆▅▆▆▆▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>kl_reg_loss</td><td>▁▁▁▁▁▁▁▁▅▅▅▅▆▅▅▅▆▅▆▆▆▅▆▆▆▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>loss</td><td>▇▆█▇▇▆▇█▆▅▄▅▄▄▄▃▅▄▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▁▁▁▁▁▁</td></tr><tr><td>pixel_loss</td><td>▇▆█▇▇▇▇█▅▄▄▄▄▄▃▃▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>recon_loss</td><td>▇▆█▇▇▇▇█▅▄▄▄▄▄▃▃▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>reg_loss</td><td>▁▁▁▁▁▁▁▁▅▅▅▅▆▅▅▅▆▅▆▆▆▅▆▆▆▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>0.0</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>kl_loss</td><td>11.04694</td></tr><tr><td>kl_reg_loss</td><td>11.04694</td></tr><tr><td>loss</td><td>31.99523</td></tr><tr><td>pixel_loss</td><td>20.94828</td></tr><tr><td>recon_loss</td><td>20.94828</td></tr><tr><td>reg_loss</td><td>11.04694</td></tr><tr><td>trainer/global_step</td><td>2349</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fancy-microwave-21</strong>: <a href=\"https://wandb.ai/iceburg/uncategorized/runs/3qdhbbfg\" target=\"_blank\">https://wandb.ai/iceburg/uncategorized/runs/3qdhbbfg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220809_145547-3qdhbbfg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3qdhbbfg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220809_152548-3ev95aiq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/3ev95aiq\" target=\"_blank\">restful-water-22</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacf97f3937b4385b444c0972e58bced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.96375, 'dci.informativeness_test': 0.2235, 'dci.disentanglement': 0.010871655247839495, 'dci.completeness': 0.009935935145965802, 'mig.discrete_score': 0.006160505930339412, 'sap.score': 9.99999999999994e-05}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=3, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "\n",
    "# AdaGVAE beta = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223e8706-9cda-492f-bd5d-008c1f9572bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3d8kupt8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">leafy-feather-9</strong>: <a href=\"https://wandb.ai/iceburg/uncategorized/runs/3d8kupt8\" target=\"_blank\">https://wandb.ai/iceburg/uncategorized/runs/3d8kupt8</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220808_002724-3d8kupt8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3d8kupt8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220808_002746-10mnd0d4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/10mnd0d4\" target=\"_blank\">colorful-bush-10</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3235645abd4f798a9996ae63324cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.9635, 'dci.informativeness_test': 0.236, 'dci.disentanglement': 0.005969988782628775, 'dci.completeness': 0.005825293230425216, 'mig.discrete_score': 0.006639468823705195, 'sap.score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=4)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c2c159-dd99-47ce-b768-8873a4099c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:10mnd0d4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>kl_loss</td><td>▅█▃▃▃▃▂▄▄▄▄▄▂▂▂▃▁▂▁▁▄▂▁▂▁▁▃▁▁▂▂▁▁▁▁▁▁▁▂▁</td></tr><tr><td>kl_reg_loss</td><td>▅█▃▃▃▃▂▄▄▄▄▄▂▂▂▃▁▂▁▁▄▂▁▂▁▁▃▁▁▂▂▁▁▁▁▁▁▁▂▁</td></tr><tr><td>loss</td><td>▃▃▅▄▆▃▄█▄▃▅▆▆▃▄▅▂▅▅██▃▆▄▄▃▆▃▅▁▅▅▃▆▄▅▄▅▂▂</td></tr><tr><td>pixel_loss</td><td>▃▃▅▄▆▃▄█▄▃▅▆▆▃▄▅▂▅▅██▃▆▄▄▃▆▃▅▁▅▅▃▆▄▅▄▅▂▂</td></tr><tr><td>recon_loss</td><td>▃▃▅▄▆▃▄█▄▃▅▆▆▃▄▅▂▅▅██▃▆▄▄▃▆▃▅▁▅▅▃▆▄▅▄▅▂▂</td></tr><tr><td>reg_loss</td><td>▅█▃▃▃▃▂▄▄▄▄▄▂▂▂▃▁▂▁▁▄▂▁▂▁▁▃▁▁▂▂▁▁▁▁▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>0.0</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>kl_loss</td><td>0.0</td></tr><tr><td>kl_reg_loss</td><td>0.0</td></tr><tr><td>loss</td><td>75.91698</td></tr><tr><td>pixel_loss</td><td>75.91698</td></tr><tr><td>recon_loss</td><td>75.91698</td></tr><tr><td>reg_loss</td><td>0.0</td></tr><tr><td>trainer/global_step</td><td>5849</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">colorful-bush-10</strong>: <a href=\"https://wandb.ai/iceburg/uncategorized/runs/10mnd0d4\" target=\"_blank\">https://wandb.ai/iceburg/uncategorized/runs/10mnd0d4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220808_002746-10mnd0d4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:10mnd0d4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220808_013130-1w0vxsyb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1w0vxsyb\" target=\"_blank\">unique-yogurt-11</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8eb302038314824a905137e3ed7a7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.9572499999999999, 'dci.informativeness_test': 0.219, 'dci.disentanglement': 0.006172441969339984, 'dci.completeness': 0.006268037868535997, 'mig.discrete_score': 0.011178546678891104, 'sap.score': 0.007000000000000003}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=3)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb072099-67ff-4f94-b9a6-c353d0868d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1w0vxsyb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>kl_loss</td><td>█▂▃▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>kl_reg_loss</td><td>█▂▃▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▆▄▇▅▅▁▆▄▃▃▇▄▆▅▄▇▃▄▃▅▅▄▅█▇▅▂▃▆▄▆▅▆▅▄▇▃▂▆▂</td></tr><tr><td>pixel_loss</td><td>▆▄▇▅▅▁▆▄▃▃▇▄▆▅▄▇▃▄▃▅▅▄▅█▇▅▂▃▆▄▆▅▆▅▄▇▃▂▆▂</td></tr><tr><td>recon_loss</td><td>▆▄▇▅▅▁▆▄▃▃▇▄▆▅▄▇▃▄▃▅▅▄▅█▇▅▂▃▆▄▆▅▆▅▄▇▃▂▆▂</td></tr><tr><td>reg_loss</td><td>█▂▃▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>0.0</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>kl_loss</td><td>3e-05</td></tr><tr><td>kl_reg_loss</td><td>9e-05</td></tr><tr><td>loss</td><td>78.36485</td></tr><tr><td>pixel_loss</td><td>78.36476</td></tr><tr><td>recon_loss</td><td>78.36476</td></tr><tr><td>reg_loss</td><td>9e-05</td></tr><tr><td>trainer/global_step</td><td>5849</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">unique-yogurt-11</strong>: <a href=\"https://wandb.ai/iceburg/uncategorized/runs/1w0vxsyb\" target=\"_blank\">https://wandb.ai/iceburg/uncategorized/runs/1w0vxsyb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220808_013130-1w0vxsyb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1w0vxsyb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220808_023353-3fsjzqxa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/3fsjzqxa\" target=\"_blank\">likely-shape-12</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63fb464822e49c982b596eb887aa5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.9509999999999998, 'dci.informativeness_test': 0.2585, 'dci.disentanglement': 0.010707287774888604, 'dci.completeness': 0.017592249995094258, 'mig.discrete_score': 0.020320297942774707, 'sap.score': 0.013299999999999996}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=2)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f91100-6b6a-459b-93cc-d5de18e5daab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220808_150249-np34hmck</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/np34hmck\" target=\"_blank\">sunny-monkey-13</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e058452ba014d0fb2dadb6ba058b20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.9632499999999999, 'dci.informativeness_test': 0.309, 'dci.disentanglement': 0.01996698214528427, 'dci.completeness': 0.027402376355425695, 'mig.discrete_score': 0.018147187608936906, 'sap.score': 0.004399999999999994}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4070652-a0d9-47da-b95a-511a6c7a6c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220809_005854-1z08335f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1z08335f\" target=\"_blank\">chocolate-fog-20</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be42e7318fe4dd1a509ba8a5fe7f67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 5\n",
      "beta was, 1\n",
      "beta is now, 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  4\n",
      "{'dci.informativeness_train': 0.9525, 'dci.informativeness_test': 0.248, 'dci.disentanglement': 0.0064043845508086296, 'dci.completeness': 0.009494685276765336, 'mig.discrete_score': 0.006008339990720148, 'sap.score': 0.007999999999999998}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1, at e=5 beta = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf418090-72cf-4fa0-86aa-6e231c775d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220809_214516-snqqmdwl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/snqqmdwl\" target=\"_blank\">misunderstood-waterfall-23</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613382bac8034a65a6b5bc6c33ab8bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 3\n",
      "beta was, 1\n",
      "beta is now, 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  4\n",
      "{'dci.informativeness_train': 0.9545, 'dci.informativeness_test': 0.2595, 'dci.disentanglement': 0.018864963501065223, 'dci.completeness': 0.026612815216192937, 'mig.discrete_score': 0.030318262422222825, 'sap.score': 0.007100000000000009}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1, at e=3 beta = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adfbcfc6-00c2-45c6-93dd-791228764508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220809_225444-3n1xnjci</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/3n1xnjci\" target=\"_blank\">playful-sun-24</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a878821fd9b74249af4309ca4044443e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 7\n",
      "beta was, 1\n",
      "beta is now, 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  4\n",
      "{'dci.informativeness_train': 0.95775, 'dci.informativeness_test': 0.2405, 'dci.disentanglement': 0.016056858295094255, 'dci.completeness': 0.016623338785882374, 'mig.discrete_score': 0.03253441363947755, 'sap.score': 0.005849999999999999}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1, at e=7 beta = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed801f52-4d62-4b73-be48-afe8d93c297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220809_235944-1tu1xd0g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1tu1xd0g\" target=\"_blank\">fancy-dust-25</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff530af1ab1245b28e54c75274e84fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 1\n",
      "beta was, 1\n",
      "beta is now, 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  4\n",
      "{'dci.informativeness_train': 0.90625, 'dci.informativeness_test': 0.22999999999999998, 'dci.disentanglement': 0.008335863746349221, 'dci.completeness': 0.00849464688215959, 'mig.discrete_score': 0.013138901224010152, 'sap.score': 0.0039000000000000163}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1, at e=1 beta = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e5726b-ae9a-436f-8a92-ddb24949ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220810_011724-2o5fpcgz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2o5fpcgz\" target=\"_blank\">lunar-cosmos-26</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc3925b142145af8c62cd05dc7ddd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 9\n",
      "beta was, 1\n",
      "beta is now, 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  4\n",
      "{'dci.informativeness_train': 0.96625, 'dci.informativeness_test': 0.258, 'dci.disentanglement': 0.034566779414661006, 'dci.completeness': 0.036886596314951836, 'mig.discrete_score': 0.04563616416504548, 'sap.score': 0.014199999999999999}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1, at e=9 beta = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0fa217-6f0e-4ef6-9b4d-908ce28a8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220811_151558-3bb3zm2j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/3bb3zm2j\" target=\"_blank\">jolly-meadow-31</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffa1d55b9d64232b2a4c5802f063ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 1\n",
      "beta was, 4\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.9572499999999999, 'dci.informativeness_test': 0.329, 'dci.disentanglement': 0.044190248253106336, 'dci.completeness': 0.046239238086364556, 'mig.discrete_score': 0.06821623848035582, 'sap.score': 0.013300000000000003}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=4)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 4, at e=1 beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe21c08-7138-48d1-9fb1-d55526fbbcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220811_161924-3edglnvd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/3edglnvd\" target=\"_blank\">faithful-universe-32</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5634233a900a4391a0d18e38be989a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 3\n",
      "beta was, 4\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.9632499999999999, 'dci.informativeness_test': 0.28200000000000003, 'dci.disentanglement': 0.019761623350170232, 'dci.completeness': 0.02609588040215255, 'mig.discrete_score': 0.013057781691484492, 'sap.score': 0.010449999999999997}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=4)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 4, at e=3 beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500079f2-2dd0-499d-b512-f3fa90725df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220811_183043-32d5g3vh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/32d5g3vh\" target=\"_blank\">rose-moon-34</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54421c6b591e48b3910a8d8f183bbf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 5\n",
      "beta was, 4\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.959, 'dci.informativeness_test': 0.2955, 'dci.disentanglement': 0.02402374064629238, 'dci.completeness': 0.033005766301051676, 'mig.discrete_score': 0.02367765810076227, 'sap.score': 0.028550000000000002}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=4)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 4, at e=5 beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5984f78d-34ab-43a4-a8fe-7aaea3176918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220811_202456-hxt0yyxf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/hxt0yyxf\" target=\"_blank\">likely-morning-35</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804b365b2a59496cb5c876973ba8d10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 7\n",
      "beta was, 4\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.9782500000000001, 'dci.informativeness_test': 0.32799999999999996, 'dci.disentanglement': 0.01821267033167238, 'dci.completeness': 0.024363697373460363, 'mig.discrete_score': 0.018021741062646687, 'sap.score': 0.014599999999999998}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=4)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 4, at e=7 beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda31630-1993-41b9-b566-fd4a45c7332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220812_001135-2cl5iges</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2cl5iges\" target=\"_blank\">eager-bird-36</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb82cc4bf9e643f292bf2184cfd89c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 9\n",
      "beta was, 4\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.80675, 'dci.informativeness_test': 0.1915, 'dci.disentanglement': 0.00587335907786125, 'dci.completeness': 0.008619202870956855, 'mig.discrete_score': 0.005614116556037368, 'sap.score': 0.0012500000000000011}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=4)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 4, at e=9 beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4506cef6-b324-4432-aabf-f3fdfcc78db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220812_031303-ik32v4yi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/ik32v4yi\" target=\"_blank\">expert-glitter-40</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75eac6a4d3fc4d14ada5ad4d08e12653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 1\n",
      "beta was, 2\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.95975, 'dci.informativeness_test': 0.257, 'dci.disentanglement': 0.011188806756418852, 'dci.completeness': 0.02560644120213104, 'mig.discrete_score': 0.004978899937053942, 'sap.score': 0.012650000000000002}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=2)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 2, at e=1 beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5eff46b-2527-4f81-baee-9caf96378640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220812_041612-3rjwg903</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/3rjwg903\" target=\"_blank\">effortless-mountain-41</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a17d3f3c944c1593dd1ca265d597a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 3\n",
      "beta was, 2\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.96475, 'dci.informativeness_test': 0.27149999999999996, 'dci.disentanglement': 0.01967200421929615, 'dci.completeness': 0.029848008727565517, 'mig.discrete_score': 0.030622950902403256, 'sap.score': 0.007550000000000001}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=2)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 2, at e=3 beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb30d1c-d25e-42f2-aee6-510a4b9a7d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220812_052105-30ty00on</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/30ty00on\" target=\"_blank\">celestial-spaceship-42</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f8d2b4eeb345aba13712ccacb9168a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 5\n",
      "beta was, 2\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.96625, 'dci.informativeness_test': 0.302, 'dci.disentanglement': 0.013626891196798432, 'dci.completeness': 0.016013949347044504, 'mig.discrete_score': 0.007126057124956649, 'sap.score': 0.006499999999999999}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=2)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 2, at e=5 beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53999b66-5aa4-4fab-8121-cd66ce7a7911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220812_094154-2o30hes7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2o30hes7\" target=\"_blank\">effortless-jazz-43</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfc7678ac354f23a8bffdeb60234b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 7\n",
      "beta was, 2\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.956, 'dci.informativeness_test': 0.327, 'dci.disentanglement': 0.04470688129698834, 'dci.completeness': 0.04341613300730843, 'mig.discrete_score': 0.05734274339976101, 'sap.score': 0.012950000000000005}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=2)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 2, at e=7 beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a147278e-03a5-4795-8d16-099b60868573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220812_115123-1w1e92tm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1w1e92tm\" target=\"_blank\">gentle-serenity-44</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489cf1e118914661b0eb44cdf14de83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 9\n",
      "beta was, 2\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.9655, 'dci.informativeness_test': 0.313, 'dci.disentanglement': 0.01029566115377221, 'dci.completeness': 0.010967841682280655, 'mig.discrete_score': 0.013032734380361847, 'sap.score': 0.015550000000000005}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=2)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 2, at e=9 beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96de2a86-3a0e-4b8d-9cd1-16b26c87c43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220812_150537-2smm81rf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2smm81rf\" target=\"_blank\">wandering-spaceship-46</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac24ca9abae7497d921d9f20e4152331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 1\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.9655, 'dci.informativeness_test': 0.29000000000000004, 'dci.disentanglement': 0.014679437523383722, 'dci.completeness': 0.019805473732933315, 'mig.discrete_score': 0.020953557496252107, 'sap.score': 0.016100000000000007}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1, at e=1 beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26534e20-f33e-4e89-9ac3-742d119f6653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220812_180153-1ekybb2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1ekybb2f\" target=\"_blank\">glamorous-yogurt-50</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b4fd6b5e2a46ef87be9def5f2e0df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 3\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.977, 'dci.informativeness_test': 0.3035, 'dci.disentanglement': 0.04560209803053397, 'dci.completeness': 0.05231574464408498, 'mig.discrete_score': 0.07017369924526681, 'sap.score': 0.013899999999999992}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset,num_workers = 128, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1, at e=3 beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a5b3c6-3576-40d9-9c91-e950fded19de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220812_224835-1yr3qvaq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1yr3qvaq\" target=\"_blank\">faithful-bush-53</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe5ee448e754631834837281805b1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 5\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.9615, 'dci.informativeness_test': 0.26, 'dci.disentanglement': 0.005728503764652066, 'dci.completeness': 0.011398844617252045, 'mig.discrete_score': 0.021745446389124505, 'sap.score': 0.0027500000000000024}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset,num_workers = 128, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1, at e=5 beta = 2 一会重跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2a16ac-b0b0-488b-b205-e5fcf8701c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220812_212218-1odl2k8x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1odl2k8x\" target=\"_blank\">morning-monkey-52</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7486df455a4ec79981c245b97ce126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 7\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.9652499999999999, 'dci.informativeness_test': 0.3185, 'dci.disentanglement': 0.0227221063486676, 'dci.completeness': 0.026959690287225863, 'mig.discrete_score': 0.04021348805593837, 'sap.score': 0.012900000000000005}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset,num_workers = 128, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1, at e=7 beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7b041c-b101-4bdf-9a04-75afa4191463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220813_012758-3qtztlkb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/3qtztlkb\" target=\"_blank\">likely-sun-54</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fd42c93d964879a54773111cdcb5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 9\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.96175, 'dci.informativeness_test': 0.2775, 'dci.disentanglement': 0.02827503692616434, 'dci.completeness': 0.031913190185128615, 'mig.discrete_score': 0.030319606993282327, 'sap.score': 0.007449999999999991}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset,num_workers = 128, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1, at e=9 beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14eec18f-4707-42dc-92e2-0809ba210cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:disent.util.inout.paths:created missing directories: /home/jovyan/data/dataset/dsprites\n",
      "INFO:disent.dataset.data._groundtruth:dsprites: data_dir_share='/home/jovyan/data/dataset/dsprites'\n",
      "DEBUG:disent.dataset.data._groundtruth:[preparing]: DataFileHashedDlH5(uri='https://raw.githubusercontent.com/deepmind/dsprites-dataset/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5', uri_name='dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5', out_name='gen.dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5') into data dir: /home/jovyan/data/dataset/dsprites\n",
      "INFO:disent.util.inout.cache:file is stale because it does not exist: '/home/jovyan/data/dataset/dsprites/gen.dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5'\n",
      "DEBUG:disent.util.inout.cache:calling wrapped function: <function DataFileHashed.prepare.<locals>.wrapped at 0x7fd6bd6c6310> because the file is stale: '/home/jovyan/data/dataset/dsprites/gen.dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5'\n",
      "INFO:disent.util.inout.cache:file is stale because it does not exist: '/home/jovyan/data/dataset/dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5'\n",
      "DEBUG:disent.util.inout.cache:calling wrapped function: <function DataFileHashed.prepare.<locals>.wrapped at 0x7fd6bd6d9160> because the file is stale: '/home/jovyan/data/dataset/dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5'\n",
      "DEBUG:disent.util.inout.files:created new temporary file: /home/jovyan/data/dataset/dsprites/.temp.d82e84de-6e46-4075-85c1-876f44229cd0.dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443\n",
      "DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 \"GET /deepmind/dsprites-dataset/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5 HTTP/1.1\" 200 28024106\n",
      "INFO:disent.util.inout.files:Downloading: https://raw.githubusercontent.com/deepmind/dsprites-dataset/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5 to: /home/jovyan/data/dataset/dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5\n",
      "Downloading: 26.7MB [00:00, 64.1MB/s]                            \n",
      "INFO:disent.util.inout.files:moved temporary file to final location: /home/jovyan/data/dataset/dsprites/.temp.d82e84de-6e46-4075-85c1-876f44229cd0.dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5 -> /home/jovyan/data/dataset/dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5\n",
      "DEBUG:disent.dataset.util.hdf5:saving h5 dataset using automatic batch size of: 23040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[IN ] entry: [1, 64, 64]        (uint8   ) \u001b[93m  4.000 \u001b[92mKiB\u001b[0m\u001b[0m chunk: [23040, 2, 4]      \u001b[33m180.000 \u001b[92mKiB\u001b[0m\u001b[0m chunks per entry: [1, 32, 16]        \u001b[33m 90.000 \u001b[93mMiB\u001b[0m\u001b[0m (\u001b[31m  512\u001b[0m)  |  compression: 'gzip' compression lvl: 4\n",
      "[OUT] entry: [1, 64, 64, 1]     (uint8   ) \u001b[93m  4.000 \u001b[92mKiB\u001b[0m\u001b[0m chunk: [1, 64, 64, 1]     \u001b[33m  4.000 \u001b[92mKiB\u001b[0m\u001b[0m chunks per entry: [1, 1, 1, 1]       \u001b[33m  4.000 \u001b[92mKiB\u001b[0m\u001b[0m (\u001b[31m    1\u001b[0m)  |  compression: 'gzip' compression lvl: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737280/737280 [00:44<00:00, 16641.34it/s]\n",
      "INFO:disent.util.inout.files:moved temporary file to final location: /home/jovyan/data/dataset/dsprites/.temp.8321fd09-384e-448b-a6a7-2502834e893e.gen.dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5 -> /home/jovyan/data/dataset/dsprites/gen.dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5\n",
      "INFO:disent.dataset.util.hdf5:[FILE SIZES] IN:  26.726 \u001b[93mMiB\u001b[0m OUT: 101.710 \u001b[93mMiB\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#  ~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~\n",
    "#  MIT License\n",
    "#\n",
    "#  Copyright (c) 2021 Nathan Juraj Michlo\n",
    "#\n",
    "#  Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "#  of this software and associated documentation files (the \"Software\"), to deal\n",
    "#  in the Software without restriction, including without limitation the rights\n",
    "#  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "#  copies of the Software, and to permit persons to whom the Software is\n",
    "#  furnished to do so, subject to the following conditions:\n",
    "#\n",
    "#  The above copyright notice and this permission notice shall be included in\n",
    "#  all copies or substantial portions of the Software.\n",
    "#\n",
    "#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "#  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "#  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "#  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "#  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "#  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "#  SOFTWARE.\n",
    "#  ~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~\n",
    "\n",
    "import logging\n",
    "\n",
    "from disent.dataset.util.datafile import DataFileHashedDlH5\n",
    "from disent.dataset.data._groundtruth import Hdf5GroundTruthData\n",
    "\n",
    "\n",
    "# ========================================================================= #\n",
    "# dataset_dsprites                                                          #\n",
    "# ========================================================================= #\n",
    "\n",
    "\n",
    "# TODO: this seems to have a memory leak compared to other datasets?\n",
    "class DSpritesData(Hdf5GroundTruthData):\n",
    "    \"\"\"\n",
    "    DSprites Dataset\n",
    "    - beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\n",
    "      (https://github.com/deepmind/dsprites-dataset)\n",
    "    Files:\n",
    "        - direct npz: https://raw.githubusercontent.com/deepmind/dsprites-dataset/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\n",
    "                      approx 2.5 GB loaded into memory\n",
    "        - direct hdf5: https://raw.githubusercontent.com/deepmind/dsprites-dataset/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5\n",
    "                       default chunk size is (23040, 2, 4), dataset is (737280, 64, 64) uint8.\n",
    "    # reference implementation: https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/data/ground_truth/dsprites.py\n",
    "    \"\"\"\n",
    "\n",
    "    name = 'dsprites'\n",
    "\n",
    "    # TODO: reference implementation has colour variants\n",
    "    factor_names = ('shape', 'scale', 'orientation', 'position_x', 'position_y')\n",
    "    factor_sizes = (3, 6, 40, 32, 32)  # TOTAL: 737280\n",
    "    img_shape = (64, 64, 1)\n",
    "\n",
    "    datafile = DataFileHashedDlH5(\n",
    "        # download file/link\n",
    "        uri='https://raw.githubusercontent.com/deepmind/dsprites-dataset/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.hdf5',\n",
    "        uri_hash={'fast': 'd6ee1e43db715c2f0de3c41e38863347', 'full': 'b331c4447a651c44bf5e8ae09022e230'},\n",
    "        # processed dataset file\n",
    "        file_hash={'fast': '25013c85aebbf4b1023d72564f9413f0', 'full': '4611d1a03e709cd5d0f6fdcdc221ca0e'},\n",
    "        # h5 re-save settings\n",
    "        hdf5_dataset_name='imgs',\n",
    "        hdf5_chunk_size=(1, 64, 64, 1),\n",
    "        hdf5_dtype='uint8',\n",
    "        hdf5_mutator=lambda x: (x * 255)[..., None],  # data is of shape (-1, 64, 64), so we add the channel dimension\n",
    "        hdf5_obs_shape=(64, 64, 1),\n",
    "    )\n",
    "\n",
    "\n",
    "# ========================================================================= #\n",
    "# END                                                                       #\n",
    "# ========================================================================= #\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    DSpritesData(prepare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61c71873-d185-40cc-b8e1-26cc0086f8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:disent.dataset.data._groundtruth:dsprites: data_dir_share='/home/jovyan/data/dataset/dsprites'\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 908 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "908 K     Trainable params\n",
      "0         Non-trainable params\n",
      "908 K     Total params\n",
      "3.634     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=0` reached.\n",
      "DEBUG:disent.metrics._dci:Generating training set.\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "DEBUG:disent.metrics._dci:Computing DCI metric.\n",
      "DEBUG:disent.metrics._mig:Generating training set.\n",
      "DEBUG:disent.metrics._sap:Generating training set.\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "DEBUG:disent.metrics._sap:Computing score matrix.\n",
      "DEBUG:disent.metrics._sap:SAP score: 0.0028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.9404, 'dci.informativeness_test': 0.18760000000000002, 'dci.disentanglement': 0.009106485591500891, 'dci.completeness': 0.01019433648783741, 'mig.discrete_score': 0.01900471147250521, 'sap.score': 0.00276}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData, DSpritesData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "#wandb.init()\n",
    "# prepare the data\n",
    "data = DSpritesData()\n",
    "#dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataset1 = Subset(DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32()),range(1,40000))\n",
    "dataloader = DataLoader(dataset=dataset1, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=3, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "#wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 0, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "\n",
    "# AdaGVAE beta = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6193145f-8ac2-4e11-a90a-e4339081bbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220813_180042-2yev910n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2yev910n\" target=\"_blank\">pleasant-butterfly-60</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 908 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "908 K     Trainable params\n",
      "0         Non-trainable params\n",
      "908 K     Total params\n",
      "3.634     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7f6d27e3f74ff7bdf87e064ea4f950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.9448000000000001, 'dci.informativeness_test': 0.27, 'dci.disentanglement': 0.010743319132933409, 'dci.completeness': 0.015594829752244773, 'mig.discrete_score': 0.012013397436693617, 'sap.score': 0.004200000000000001}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData, DSpritesData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = DSpritesData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataset1 = Subset(DisentDataset(data, transform=ToImgTensorF32()),range(0,40000))\n",
    "dataloader = DataLoader(dataset=dataset1, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=4)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fafed73f-5746-4bdd-9199-7bce90100e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2yev910n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>kl_loss</td><td>▁▁██▇▇█▇███████▇███▇██▇█████▇▇▇█▇███▇██▇</td></tr><tr><td>kl_reg_loss</td><td>▁▁██▇▇█▇███████▇███▇██▇█████▇▇▇█▇███▇██▇</td></tr><tr><td>loss</td><td>██▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>pixel_loss</td><td>██▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recon_loss</td><td>██▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reg_loss</td><td>▁▁██▇▇█▇███████▇███▇██▇█████▇▇▇█▇███▇██▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>0.0</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>kl_loss</td><td>5.23479</td></tr><tr><td>kl_reg_loss</td><td>20.93917</td></tr><tr><td>loss</td><td>42.46182</td></tr><tr><td>pixel_loss</td><td>21.52265</td></tr><tr><td>recon_loss</td><td>21.52265</td></tr><tr><td>reg_loss</td><td>20.93917</td></tr><tr><td>trainer/global_step</td><td>3099</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pleasant-butterfly-60</strong>: <a href=\"https://wandb.ai/iceburg/uncategorized/runs/2yev910n\" target=\"_blank\">https://wandb.ai/iceburg/uncategorized/runs/2yev910n</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220813_180042-2yev910n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2yev910n). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220813_183307-1hi3ouuk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1hi3ouuk\" target=\"_blank\">fallen-salad-61</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 908 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "908 K     Trainable params\n",
      "0         Non-trainable params\n",
      "908 K     Total params\n",
      "3.634     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8853dfe03e9b46febffe864cd6dff97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.9414, 'dci.informativeness_test': 0.2956, 'dci.disentanglement': 0.01580334537603302, 'dci.completeness': 0.021859942864444044, 'mig.discrete_score': 0.026773211423763828, 'sap.score': 0.024800000000000006}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData, DSpritesData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = DSpritesData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataset1 = Subset(DisentDataset(data, transform=ToImgTensorF32()),range(0,40000))\n",
    "dataloader = DataLoader(dataset=dataset1, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=3)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec07935-b25d-4199-9073-1aa43a3204be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220813_195548-2howx7k1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2howx7k1\" target=\"_blank\">proud-field-62</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 908 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "908 K     Trainable params\n",
      "0         Non-trainable params\n",
      "908 K     Total params\n",
      "3.634     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1531e1c0774c4599be20b7d3d080c61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.9398, 'dci.informativeness_test': 0.2964, 'dci.disentanglement': 0.007453266502894405, 'dci.completeness': 0.008843422304309788, 'mig.discrete_score': 0.010728792550681498, 'sap.score': 0.027800000000000002}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData, DSpritesData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = DSpritesData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataset1 = Subset(DisentDataset(data, transform=ToImgTensorF32()),range(0,40000))\n",
    "dataloader = DataLoader(dataset=dataset1, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=2)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec03bdf-ec24-43db-9d01-fbf1faeefa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220813_205241-3acdworu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/3acdworu\" target=\"_blank\">mild-plant-63</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 908 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "908 K     Trainable params\n",
      "0         Non-trainable params\n",
      "908 K     Total params\n",
      "3.634     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675329dffe6e40e9b3483c6465ce64e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.9443999999999999, 'dci.informativeness_test': 0.27840000000000004, 'dci.disentanglement': 0.007064775821406678, 'dci.completeness': 0.011557220653987432, 'mig.discrete_score': 0.008483469396094914, 'sap.score': 0.013999999999999995}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData, DSpritesData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = DSpritesData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataset1 = Subset(DisentDataset(data, transform=ToImgTensorF32()),range(0,40000))\n",
    "dataloader = DataLoader(dataset=dataset1, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab3ceab-4c84-40d0-896b-77cdff03ef14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 908 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "908 K     Trainable params\n",
      "0         Non-trainable params\n",
      "908 K     Total params\n",
      "3.634     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072011a4087740d2922457de1031e72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 1\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.952, 'dci.informativeness_test': 0.29840000000000005, 'dci.disentanglement': 0.01067420503011551, 'dci.completeness': 0.013610852883915234, 'mig.discrete_score': 0.014443691467473782, 'sap.score': 0.020240000000000005}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData, DSpritesData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "#wandb.init()\n",
    "# prepare the data\n",
    "data = DSpritesData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataset1 = Subset(DisentDataset(data, transform=ToImgTensorF32()),range(0,40000))\n",
    "dataloader = DataLoader(dataset=dataset1, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger,callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1 at e=1 beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1484c7-395e-4b9a-8b52-6578d38a8d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220814_025957-1v2vjmgz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1v2vjmgz\" target=\"_blank\">dark-disco-65</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 908 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "908 K     Trainable params\n",
      "0         Non-trainable params\n",
      "908 K     Total params\n",
      "3.634     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968086c796124c43ad2213f6bf1e5e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 3\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.9432, 'dci.informativeness_test': 0.3228, 'dci.disentanglement': 0.06487938849052095, 'dci.completeness': 0.06673431709145436, 'mig.discrete_score': 0.04730935350770561, 'sap.score': 0.03764000000000001}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData, DSpritesData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = DSpritesData()\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataset1 = Subset(DisentDataset(data, transform=ToImgTensorF32()),range(0,40000))\n",
    "dataloader = DataLoader(dataset=dataset1, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer='adam', optimizer_kwargs=dict(lr=1e-3), loss_reduction='mean_sum', beta=1)\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger,callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# BetaVAE beta = 1 at e=3 beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a9dcd1-143b-4ea7-81c6-ae272cea34c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220814_185039-1ae6eykc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1ae6eykc\" target=\"_blank\">firm-cherry-67</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb6f14db46745f98bcaa0240fc84e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dci.informativeness_train': 0.9875, 'dci.informativeness_test': 0.6655, 'dci.disentanglement': 0.2374377488697766, 'dci.completeness': 0.19478966337021464, 'mig.discrete_score': 0.21664921208308005, 'sap.score': 0.01485}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=0, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"validation.ipynb\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ffbc87-78a3-4bdc-8a1f-ac07737dac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220815_180159-1j27ffi4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1j27ffi4\" target=\"_blank\">genial-armadillo-1</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e12bc45d391456982ebb17c5704c3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 1\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.913, 'dci.informativeness_test': 0.20199999999999999, 'dci.disentanglement': 0.008324600184776315, 'dci.completeness': 0.010495640459921322, 'mig.discrete_score': 0.008899098587702162, 'sap.score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=1, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 1 at e=1, beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236b43be-617f-4b8f-8735-331dc6ffc2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220815_234148-2gddjl1t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2gddjl1t\" target=\"_blank\">pretty-puddle-7</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f5c0bdf6324e4284d7b711cc7bf46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 3\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.97675, 'dci.informativeness_test': 0.673, 'dci.disentanglement': 0.5025637093809852, 'dci.completeness': 0.5769417514799695, 'mig.discrete_score': 0.4873825306087149, 'sap.score': 0.015100000000000008}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=1, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 1 at e=3, beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72644760-395f-40b5-8871-7f65208dc0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220816_044432-1tgvmifi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1tgvmifi\" target=\"_blank\">chocolate-wood-8</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f456096664047c3bf00317daa990641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 5\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.9765, 'dci.informativeness_test': 0.6855, 'dci.disentanglement': 0.5314044110791448, 'dci.completeness': 0.557302093311387, 'mig.discrete_score': 0.4585289565787226, 'sap.score': 0.0232}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=1, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 1 at e=5, beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8cec1d-4d38-4451-89c8-2bf963e41f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220816_101356-1vgdgpyd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1vgdgpyd\" target=\"_blank\">stellar-totem-9</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40fb83d235440a18016d86c7a121ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 7\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.97875, 'dci.informativeness_test': 0.7124999999999999, 'dci.disentanglement': 0.5682242378857836, 'dci.completeness': 0.6198013990956245, 'mig.discrete_score': 0.4851480340785487, 'sap.score': 0.03545}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=1, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 1 at e=7, beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98db5bd-b91f-4faf-a7fd-3a8d58fe1ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220816_124556-378i1f76</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/378i1f76\" target=\"_blank\">sandy-elevator-10</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218d198cff85411594220acc6a8199ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 9\n",
      "beta was, 1\n",
      "beta is now, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  2\n",
      "{'dci.informativeness_train': 0.977, 'dci.informativeness_test': 0.6739999999999999, 'dci.disentanglement': 0.5273395192425567, 'dci.completeness': 0.5735073687737746, 'mig.discrete_score': 0.4557159689149634, 'sap.score': 0.030800000000000008}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=1, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 1 at e=9, beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a823c78-f5d4-4b3d-b07f-59d0880c9ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220816_152909-296ou6eq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/296ou6eq\" target=\"_blank\">azure-surf-11</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58454e26277142b6af8462cdcaf88940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 1\n",
      "beta was, 2\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.96975, 'dci.informativeness_test': 0.6440000000000001, 'dci.disentanglement': 0.4710847500972034, 'dci.completeness': 0.5294901956193412, 'mig.discrete_score': 0.4406668568409308, 'sap.score': 0.01465}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=2, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 2 at e=1, beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad9cbb5-0889-496a-84b5-1e0b2baf8914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220816_173016-1n7hc4sp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/1n7hc4sp\" target=\"_blank\">dandy-pine-12</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9885e2aed8ab47a49eed367f535c7926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 5\n",
      "beta was, 0\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.99825, 'dci.informativeness_test': 0.8450000000000001, 'dci.disentanglement': 0.4133796563133506, 'dci.completeness': 0.34639792008111236, 'mig.discrete_score': 0.14527725061784266, 'sap.score': 0.006149999999999999}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=0, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 0 at e=5, beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c28ae5-6cbb-4ea6-b681-dc9c1f278088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220816_213943-36ms0nt7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/36ms0nt7\" target=\"_blank\">divine-armadillo-13</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc78eaa88924ce9a9e396c4d2fa004d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 5\n",
      "beta was, 1\n",
      "beta is now, 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  0\n",
      "{'dci.informativeness_train': 0.98575, 'dci.informativeness_test': 0.777, 'dci.disentanglement': 0.5336665616375376, 'dci.completeness': 0.48562076827643785, 'mig.discrete_score': 0.36161075332609516, 'sap.score': 0.016950000000000007}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=1, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 1 at e=5, beta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cea6ce-dee3-4c6d-aead-073fe13a78cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220817_020104-2cjv3rrw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2cjv3rrw\" target=\"_blank\">easy-serenity-14</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5696c3ec51ab4cf7ab21c9cab5c6f371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 3\n",
      "beta was, 2\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.9695, 'dci.informativeness_test': 0.6155, 'dci.disentanglement': 0.3455165704779406, 'dci.completeness': 0.3418883665189123, 'mig.discrete_score': 0.1818126425060103, 'sap.score': 0.016600000000000007}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=2, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 2 at e=3, beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3d432e-0622-4acb-8e85-218f04b64aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220817_041523-2niaqn8y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2niaqn8y\" target=\"_blank\">autumn-shape-15</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9b5ccd62034efa8df754d478bcce2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 5\n",
      "beta was, 2\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.968, 'dci.informativeness_test': 0.462, 'dci.disentanglement': 0.18318570226026765, 'dci.completeness': 0.17067280835321372, 'mig.discrete_score': 0.10233032107411798, 'sap.score': 0.02085000000000001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=2, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 2 at e=5, beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432281a9-c804-48ab-83a7-4ceef29402f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220817_100151-2o9o8uox</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2o9o8uox\" target=\"_blank\">glorious-armadillo-16</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361ec8340f38478d8893c8d18048c118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 7\n",
      "beta was, 2\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.95625, 'dci.informativeness_test': 0.2275, 'dci.disentanglement': 0.011082880679038436, 'dci.completeness': 0.009629510037555272, 'mig.discrete_score': 0.009393038458709943, 'sap.score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=2, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 2 at e=7, beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208ef3d4-26a1-4fe9-8d26-ba0aa71862bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220817_123725-ql2g235h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/ql2g235h\" target=\"_blank\">grateful-spaceship-17</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbe722e1dac46b9b8e09715ce61fd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 9\n",
      "beta was, 2\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.96275, 'dci.informativeness_test': 0.5760000000000001, 'dci.disentanglement': 0.32345104416905773, 'dci.completeness': 0.3451440374172538, 'mig.discrete_score': 0.1983131710741087, 'sap.score': 0.0049500000000000065}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=2, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 2 at e=9, beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac38bdb-f35d-48e8-a212-7ea78bed677b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:20p146o3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆███</td></tr><tr><td>kl_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>kl_reg_loss</td><td>█▇▆▆▄▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▄▃▄▄▃▅▂▅▄▄▅▂▅▅▃▃▄▆▃▃▄▃▃▁▃█▁▇▁▂▄▅▇▃▁▂▃▄▁▁</td></tr><tr><td>pixel_loss</td><td>▄▃▄▄▃▅▂▅▄▄▅▂▅▅▃▃▄▆▃▃▄▃▃▁▃█▁▇▁▂▄▅▇▃▁▂▃▄▁▁</td></tr><tr><td>recon_loss</td><td>▄▃▄▄▃▅▂▅▄▄▅▂▅▅▃▃▄▆▃▃▄▃▃▁▃█▁▇▁▂▄▅▇▃▁▂▃▄▁▁</td></tr><tr><td>reg_loss</td><td>█▇▆▆▄▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>shared</td><td>▄█▃▄▅▅▅▃▃▃▃▅▅▆▄▆▁▂▁▅▄▅▄▃▃▂▅▄▃▄▄▄▇▂▅▄▇▃▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>0.0</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>kl_loss</td><td>10.27122</td></tr><tr><td>kl_reg_loss</td><td>0.0</td></tr><tr><td>loss</td><td>78.1335</td></tr><tr><td>pixel_loss</td><td>78.1335</td></tr><tr><td>recon_loss</td><td>78.1335</td></tr><tr><td>reg_loss</td><td>0.0</td></tr><tr><td>shared</td><td>3.96875</td></tr><tr><td>trainer/global_step</td><td>2499</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lilac-jazz-18</strong>: <a href=\"https://wandb.ai/iceburg/uncategorized/runs/20p146o3\" target=\"_blank\">https://wandb.ai/iceburg/uncategorized/runs/20p146o3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220817_150640-20p146o3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:20p146o3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220817_161220-iiyp6uop</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/iiyp6uop\" target=\"_blank\">autumn-wind-19</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42a4014f75547bb94fb77791ce75083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 1\n",
      "beta was, 1\n",
      "beta is now, 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  0\n",
      "{'dci.informativeness_train': 0.98125, 'dci.informativeness_test': 0.672, 'dci.disentanglement': 0.24214961621462538, 'dci.completeness': 0.1996359458164787, 'mig.discrete_score': 0.23532494574945279, 'sap.score': 0.029500000000000005}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=1, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 1 at e=1, beta = 0 5有了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de59d692-3826-42ea-80c8-d46d08f6d905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1two0o41) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆████</td></tr><tr><td>kl_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>kl_reg_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▅▅▆▂▂▂▁▆▆▃▃▅▂▄▂▅▆▄▁▃▄▇▃▅▆▂▃▃▄▂▃▄▄▁▆</td></tr><tr><td>pixel_loss</td><td>█▅▃▃▃▅▅▆▂▂▂▁▆▆▃▃▅▂▄▂▅▆▄▁▃▄▇▃▅▆▂▃▃▄▂▃▄▄▁▆</td></tr><tr><td>recon_loss</td><td>█▅▃▃▃▅▅▆▂▂▂▁▆▆▃▃▅▂▄▂▅▆▄▁▃▄▇▃▅▆▂▃▃▄▂▃▄▄▁▆</td></tr><tr><td>reg_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>shared</td><td>▇▇▄▃▃▃▁▂▂▃▂▂▂▄▁▃▁▂▂▂▁▂▃▁▃▃▁▅▆▆▅▆██▇▇▇▆▆▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>aug_loss</td><td>0.0</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>kl_loss</td><td>3.31321</td></tr><tr><td>kl_reg_loss</td><td>0.0</td></tr><tr><td>loss</td><td>97.33907</td></tr><tr><td>pixel_loss</td><td>97.33907</td></tr><tr><td>recon_loss</td><td>97.33907</td></tr><tr><td>reg_loss</td><td>0.0</td></tr><tr><td>shared</td><td>3.07812</td></tr><tr><td>trainer/global_step</td><td>2599</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">young-cherry-20</strong>: <a href=\"https://wandb.ai/iceburg/uncategorized/runs/1two0o41\" target=\"_blank\">https://wandb.ai/iceburg/uncategorized/runs/1two0o41</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220817_182651-1two0o41/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1two0o41). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220817_202407-27h5freu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/27h5freu\" target=\"_blank\">crisp-armadillo-21</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7da3ad068d4bce921cbc66256dc0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 3\n",
      "beta was, 1\n",
      "beta is now, 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  0\n",
      "{'dci.informativeness_train': 0.979, 'dci.informativeness_test': 0.749, 'dci.disentanglement': 0.3881731932500562, 'dci.completeness': 0.32677987681490056, 'mig.discrete_score': 0.25746161816279073, 'sap.score': 0.03065}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=1, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 1 at e=3, beta = 0 5有了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c48679d-3cdf-4d68-9e0b-80b29e4ffc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220817_223349-2avlpabc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/2avlpabc\" target=\"_blank\">kind-breeze-22</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2c0775466a4c5db3fffbc555dd5f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 7\n",
      "beta was, 1\n",
      "beta is now, 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  0\n",
      "{'dci.informativeness_train': 0.9715, 'dci.informativeness_test': 0.6345000000000001, 'dci.disentanglement': 0.38012774579468395, 'dci.completeness': 0.4008640526950967, 'mig.discrete_score': 0.3217035148044986, 'sap.score': 0.02229999999999999}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=1, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 1 at e=7, beta = 0 5有了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87eecf0-5598-4617-b7b0-0a3f67b47ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220818_010307-p4g8olqp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/p4g8olqp\" target=\"_blank\">spring-pond-23</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9604ce13b90244c4af3bdf790d646630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 9\n",
      "beta was, 1\n",
      "beta is now, 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  0\n",
      "{'dci.informativeness_train': 0.992, 'dci.informativeness_test': 0.7765000000000001, 'dci.disentanglement': 0.5177135363332566, 'dci.completeness': 0.5043791851253021, 'mig.discrete_score': 0.2952711240717714, 'sap.score': 0.02780000000000001}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=1, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 1 at e=9, beta = 0 5有了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd7c7b9-a64b-49bd-9aa6-ae7e9e32bf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220818_030547-ehe4865p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/ehe4865p\" target=\"_blank\">generous-elevator-24</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc9ba02924c41d191b77f1e6d4320aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 1\n",
      "beta was, 0\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.97975, 'dci.informativeness_test': 0.7475, 'dci.disentanglement': 0.44892538107491864, 'dci.completeness': 0.47912099331444696, 'mig.discrete_score': 0.2883159063105663, 'sap.score': 0.043000000000000003}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=0, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 0 at e=1, beta = 1 5有了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d854b5ec-9cc2-49f2-95dd-7e235d220409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220818_054406-ii40tjuc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/ii40tjuc\" target=\"_blank\">feasible-tree-25</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec379411c6f4419a73a1487007ab455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 3\n",
      "beta was, 0\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.9652499999999999, 'dci.informativeness_test': 0.662, 'dci.disentanglement': 0.27075096628862394, 'dci.completeness': 0.22816474307507986, 'mig.discrete_score': 0.0631221718553924, 'sap.score': 0.013850000000000001}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=0, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 0 at e=3, beta = 1 5有了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b262bfb-aead-4644-90ee-f7fa49f6f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220818_111139-3tnxetgg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/3tnxetgg\" target=\"_blank\">amber-star-26</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0400226a23643848dac611921e9fc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 7\n",
      "beta was, 0\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.99475, 'dci.informativeness_test': 0.8865, 'dci.disentanglement': 0.6819841219157023, 'dci.completeness': 0.6519969000755033, 'mig.discrete_score': 0.4758582514190737, 'sap.score': 0.011800000000000003}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=0, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 0 at e=7, beta = 1 5有了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37b13d6-22e9-4977-8654-6dbaa2db72c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miceburg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20220818_143043-39jjzuxw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iceburg/uncategorized/runs/39jjzuxw\" target=\"_blank\">sandy-galaxy-27</a></strong> to <a href=\"https://wandb.ai/iceburg/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:351: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                          | Type                | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | _model                        | AutoEncoder         | 910 K \n",
      "1 | _AeAndVaeMixin__recon_handler | ReconLossHandlerMse | 0     \n",
      "----------------------------------------------------------------------\n",
      "910 K     Trainable params\n",
      "0         Non-trainable params\n",
      "910 K     Total params\n",
      "3.643     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2790fbc17d9a43ef8c5e6b18f05ab142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/base.py:247: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.current_epoch is now 9\n",
      "beta was, 0\n",
      "beta is now, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train end and beta NOW =  1\n",
      "{'dci.informativeness_train': 0.9615, 'dci.informativeness_test': 0.6535, 'dci.disentanglement': 0.2579765505708988, 'dci.completeness': 0.26653818843745913, 'mig.discrete_score': 0.19755160396858237, 'sap.score': 0.010899999999999996}\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData\n",
    "from disent.dataset.sampling import GroundTruthPairOrigSampler\n",
    "from disent.frameworks.ae import Ae\n",
    "from disent.frameworks.vae import BetaVae, AdaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64, EncoderConv64\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "from disent.metrics import metric_dci, metric_mig, metric_sap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import callback as tc\n",
    "\n",
    "wandb.init()\n",
    "# prepare the data\n",
    "data = XYObjectData()\n",
    "dataset = DisentDataset(data, GroundTruthPairOrigSampler(), transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: pl.LightningModule = AdaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=6, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=6),\n",
    "    ),\n",
    "    cfg=AdaVae.cfg(\n",
    "        optimizer='adam', optimizer_kwargs=dict(lr=1e-3),\n",
    "        loss_reduction='mean_sum', beta=0, ada_average_mode='gvae', ada_thresh_mode='kl',\n",
    "    )\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"beta-VAE\")\n",
    "# train the model\n",
    "trainer = pl.Trainer(logger=wandb_logger, callbacks=[tc.betaControlCallback()], max_epochs = 10, fast_dev_run=is_test_run())\n",
    "trainer.fit(module, dataloader)\n",
    "\n",
    "module.eval()\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "a_results = {\n",
    "        **metric_dci(dataset, get_repr, num_train=10 if is_test_run() else 1000, num_test=5 if is_test_run() else 500, boost_mode='sklearn'),\n",
    "        **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "        **metric_sap(dataset, get_repr),\n",
    "    }\n",
    "print(a_results)\n",
    "# AdaGVAE beta = 0 at e=9, beta = 1 5有了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a9148-b285-491a-8eb6-5b02a5e74a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
